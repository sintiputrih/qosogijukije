<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><title>Previewing GeForce RTX 2080 Ti -</title><meta name=robots content="index,follow,noarchive"><meta name=description content="Turing and NVIDIA’s focus on hybrid rendering aside, let’s take a look at the individual GeForce RTX cards.
Before getting too far here, it’s important to point out that NVIDIA has offered little in the way of information on the cards’ performance besides their formal specifications. Essentially the entirety of the NVIDIA Gamescom presentation – and even most of the SIGGRAPH presentation – was focused on ray tracing/hybrid rendering and the Turing architecture’s unique hardware capabilities to support those features."><meta name=author content="Larita Shotwell"><link rel="preload stylesheet" as=style href=https://assets.cdnweb.info/hugo/paper/css/app.css><link rel="preload stylesheet" as=style href=https://assets.cdnweb.info/hugo/paper/css/an-old-hope.min.css><script defer src=https://assets.cdnweb.info/hugo/paper/js/highlight.min.js onload=hljs.initHighlightingOnLoad()></script>
<link rel=preload as=image href=./theme.png><link rel=icon href=./favicon.ico><link rel=apple-touch-icon href=./apple-touch-icon.png><meta name=generator content="Hugo 0.98.0"><meta property="og:title" content="Previewing GeForce RTX 2080 Ti"><meta property="og:description" content="Turing and NVIDIAs focus on hybrid rendering aside, lets take a look at the individual GeForce RTX cards. Before getting too far here, its important to point out that NVIDIA has offered little in the way of information on the cards performance besides their formal specifications. Essentially the entirety of the NVIDIA Gamescom presentation "><meta property="og:type" content="article"><meta property="og:url" content="/nvidia-announces-geforce-rtx-20-series-rtx-2080-ti-2080-2070.html"><meta property="article:section" content="post"><meta property="article:published_time" content="2024-09-27T00:00:00+00:00"><meta property="article:modified_time" content="2024-09-27T00:00:00+00:00"><meta itemprop=name content="Previewing GeForce RTX 2080 Ti"><meta itemprop=description content="Turing and NVIDIAs focus on hybrid rendering aside, lets take a look at the individual GeForce RTX cards. Before getting too far here, its important to point out that NVIDIA has offered little in the way of information on the cards performance besides their formal specifications. Essentially the entirety of the NVIDIA Gamescom presentation "><meta itemprop=datePublished content="2024-09-27T00:00:00+00:00"><meta itemprop=dateModified content="2024-09-27T00:00:00+00:00"><meta itemprop=wordCount content="2102"><meta itemprop=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="Previewing GeForce RTX 2080 Ti"><meta name=twitter:description content="Turing and NVIDIAs focus on hybrid rendering aside, lets take a look at the individual GeForce RTX cards. Before getting too far here, its important to point out that NVIDIA has offered little in the way of information on the cards performance besides their formal specifications. Essentially the entirety of the NVIDIA Gamescom presentation "></head><body class=not-ready data-menu=true><header class=header><p class=logo><a class=site-name href=./index.html>BlinkDash</a><a class=btn-dark></a></p><script>let bodyClx=document.body.classList,btnDark=document.querySelector(".btn-dark"),sysDark=window.matchMedia("(prefers-color-scheme: dark)"),darkVal=localStorage.getItem("dark"),setDark=e=>{bodyClx[e?"add":"remove"]("dark"),localStorage.setItem("dark",e?"yes":"no")};setDark(darkVal?darkVal==="yes":sysDark.matches),requestAnimationFrame(()=>bodyClx.remove("not-ready")),btnDark.addEventListener("click",()=>setDark(!bodyClx.contains("dark"))),sysDark.addEventListener("change",e=>setDark(e.matches))</script><nav class=menu><a href=./sitemap.xml>Sitemap</a></nav></header><main class=main><article class=post-single><header class=post-title><p><time>Sep 27, 2024</time>
<span>Larita Shotwell</span></p><h1>Previewing GeForce RTX 2080 Ti</h1></header><section class=post-content><p>Turing and NVIDIA’s focus on hybrid rendering aside, let’s take a look at the individual GeForce RTX cards.</p><p>Before getting too far here, it’s important to point out that NVIDIA has offered little in the way of information on the cards’ performance besides their formal specifications. Essentially the entirety of the NVIDIA Gamescom presentation – and even most of the SIGGRAPH presentation – was focused on ray tracing/hybrid rendering and the Turing architecture’s unique hardware capabilities to support those features. As a result we don’t have a good frame of reference for how these specifications will translate into real-world performance. Which is also why we’re disappointed that NVIDIA has already started pre-orders, as it pushes consumers into blindly buying cards.</p><p>At any rate, with NVIDIA having changed the SM for Turing as much as they have versus Pascal, I don’t believe FLOPS alone is an accurate proxy for performance in current games. It’s almost certain that NVIDIA has been able to improve their SM efficiency, especially judging from what we’ve seen thus far with the Titan V. So in that respect this launch is similar to the Maxwell launch in that the raw specifications can be deceiving, and that it’s possible to lose FLOPS and still gain performance.</p><p align=center><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/13249/RTX_1080ti_Car_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p>In any case, at the top of the GeForce RTX 20 series stack will be the GeForce RTX 2080 Ti. A major departure from the GeForce 700/900/10 series, NVIDIA is not retaining the Ti card as a mid-generation kicker. Instead they’re launching with it right away. This means that the high-end of the RTX family is now a 3 card stack from the start, instead of a 2 card stack as has previously been the case.</p><p>NVIDIA has not commented on this change in particular, and this is one of those things that I expect we’ll know more about once we reach the actual hardware launch. But there’s good reason to suspect that since NVIDIA is using the relatively mature TSMC 12nm “FFN” process – itself an optimized version of 16nm – that yields are in a better place than usual at this time. Normally NVIDIA would be using a more bleeding-edge process, where it would make sense to hold back the largest chip another year or so to let yields improve.</p><table align=center border=1 cellpadding=3 cellspacing=0 width=650><tbody readability=3><tr class=tgrey readability=2><td align=center colspan=7>NVIDIA GeForce x80 Ti Specification Comparison</td></tr><tr class=tlblue readability=2><td align=center class=contentwhite width=168>&nbsp;</td><td align=center class=contentwhite width=110>RTX 2080 Ti<br>Founder's Edition</td><td align=center class=contentwhite width=110>RTX 2080 Ti</td><td align=center class=contentwhite width=110>GTX 1080 Ti</td><td align=center class=contentwhite width=110>GTX 980 Ti</td></tr><tr><td align=left class=tlgrey><strong>CUDA Cores</strong></td><td align=center>4352</td><td align=center>4352</td><td align=center>3584</td><td align=center>2816</td></tr><tr><td align=left class=tlgrey><strong>ROPs</strong></td><td align=center>88?</td><td align=center>88?</td><td align=center>88</td><td align=center>96</td></tr><tr><td align=left class=tlgrey><strong>Core Clock</strong></td><td align=center>1350MHz</td><td align=center>1350MHz</td><td align=center>1481MHz</td><td align=center>1000MHz</td></tr><tr><td align=left class=tlgrey><strong>Boost Clock</strong></td><td align=center>1635MHz</td><td align=center>1545MHz</td><td align=center>1582MHz</td><td align=center>1075MHz</td></tr><tr><td align=left class=tlgrey><strong>Memory Clock</strong></td><td align=center>14Gbps GDDR6</td><td align=center>14Gbps GDDR6</td><td align=center>11Gbps GDDR5X</td><td align=center>7Gbps GDDR5</td></tr><tr><td align=left class=tlgrey><strong>Memory Bus Width</strong></td><td align=center>352-bit</td><td align=center>352-bit</td><td align=center>352-bit</td><td align=center>384-bit</td></tr><tr><td align=left class=tlgrey><strong>VRAM</strong></td><td align=center>11GB</td><td align=center>11GB</td><td align=center>11GB</td><td align=center>6GB</td></tr><tr><td align=left class=tlgrey><strong>Single Precision Perf.</strong></td><td align=center>14.2 TFLOPs</td><td align=center>13.4 TFLOPs</td><td align=center>11.3 TFLOPs</td><td align=center>6.1 TFLOPs</td></tr><tr><td align=left class=tlgrey><strong>"RTX-OPS"</strong></td><td align=center>78T</td><td align=center>78T</td><td align=center>N/A</td><td align=center>N/A</td></tr><tr><td align=left class=tlgrey><strong>TDP</strong></td><td align=center>260W</td><td align=center>250W</td><td align=center>250W</td><td align=center>250W</td></tr><tr><td align=left class=tlgrey><strong>GPU</strong></td><td align=center>Big Turing</td><td align=center>Big Turing</td><td align=center>GP102</td><td align=center>GM200</td></tr><tr><td align=left class=tlgrey><strong>Architecture</strong></td><td align=center>Turing</td><td align=center>Turing</td><td align=center>Pascal</td><td align=center>Maxwell</td></tr><tr><td align=left class=tlgrey><strong>Manufacturing Process</strong></td><td align=center>TSMC 12nm "FFN"</td><td align=center>TSMC 12nm "FFN"</td><td align=center>TSMC 16nm</td><td align=center>TSMC 28nm</td></tr><tr><td align=left class=tlgrey><strong>Launch Date</strong></td><td align=center>09/20/2018</td><td align=center>09/20/2018</td><td align=center>03/10/2017</td><td align=center>06/01/2015</td></tr><tr readability=2><td align=left class=tlgrey><strong>Launch Price</strong></td><td align=center>$1199</td><td align=center>$999</td><td align=center>MSRP: $699<br>Founders: $699</td><td align=center>$649</td></tr></tbody></table><p>The king of NVIDIA’s new product stack, the GeForce RTX 2080 Ti is without a doubt an interesting card. And if we’re being honest, it’s not a card I was expecting. Based on these specifications, it’s clearly built around a cut-down version of NVIDIA’s “Big Turing” GPU, which the company just unveiled last week at SIGGRAPH. And like the name suggests, Big Turing is big: 18.6B transistors, measuring 754mm2 in die size. This is closer in size to GV100 (Volta/Titan V) than it is any past x80 Ti card, so I am surprised that, even as a cut-down chip, NVIDIA can economically offer it for sale. None the less here we are, with Big Turing coming to consumer cards.</p><p>Even though it’s a cut-down part, RTX 2080 Ti is still a beast, with 4352 Turing CUDA cores and what I estimate to be 544 tensor cores. Like its Quadro counterpart, this card is rated for 10 GigaRays/second, and for traditional compute we’re looking at 13.4 TFLOPS based on these specifications. Note that this is only 19% higher than GTX 1080 Ti, which is all the more reason why I want to learn more about Turing’s architectural changes before predicting what this means for performance in current-generation rasterization games.</p><p>Clockspeeds have actually dropped from generation to generation here. Whereas the GTX 1080 Ti started at 1.48GHz and had an official boost clock rating of 1.58GHz (and in practice boosting higher still), RTX 2080 Ti starts at 1.35GHz and boosts to 1.55GHz, while we don’t know anything about the practical boost limits. So assuming NVIDIA is being as equally conservative as the last generation, then this means the average clockspeeds have dropped slightly. Which in turn means that whatever performance gains we see from GTX 2080 Ti are going to ride entirely on the increased CUDA core count and any architectural efficiency improvements.</p><p align=center><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/13249/2080ti_back_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p>Meanwhile the ROP count is unknown, but as it needs to match the memory bus width, we’re almost certainly looking at 88 ROPs. Even more so than the core compute architecture, I’m curious as to whether there are any architectural improvements here. Otherwise because the ROP count is identical, then the maximum pixel throughput (on paper) is actually ever so slightly lower than it was on GTX 1080 Ti.</p><p>Speaking of the memory bus, this is another area that is seeing a significant improvement. NVIDIA has moved from GDDR5X to GDDR6, so memory clockspeeds have increased accordingly, from 11Gbps to 14Gbps, a 27% increase. And since the memory bus width itself remains identical at 352-bits wide, this means the final memory bandwidth increase is also 27%. Memory bandwidth has long been the Achilles heel of GPUs, so even if NVIDIA’s theoretical ROP throughput has not changed this generation, the fact of the matter is that having more memory bandwidth is going to remove bottlenecks and improve performance throughout the rendering pipeline, from the texture units and CUDA cores straight out to the ROPs. Of course, the tensor cores and RT cores are going to be prolific bandwidth consumers as well, so in workloads where they’re in play, NVIDIA is once again going to have to do more with (relatively) less.</p><p>Past this, things start diverging a bit. NVIDIA is once again offering their reference-grade Founders Edition cards, and unlike with the GeForce 10 series, the 20 series FE cards have slightly different specifications than their base specification compatriots. Specifically, NVIDIA has cranked up the clockspeed and the resulting TDP a bit, giving the 2080 Ti FE an on-paper 6% performance advantage, and also a 10W higher TDP. For the standard cards then, the TDP is the x80 Ti-traditional 250W, while the FE card moves to 260W.</p><p align=center><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/13249/2080ti_top_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p>Meanwhile, starting with the GeForce 20 series cards, NVIDIA is rolling out a new design to their reference/Founders Edition cards, the first such redesign since the original GeForce GTX Titan back in 2013. Up until now NVIDIA has focused on a conservative but highly effective blower design, pairing the best blower in the industry with a metal grey & black metal shroud. The end result is that these reference/FE cards could be dropped in virtually any system and work, thanks to the self-exhausting nature of blowers.</p><p>However for the GeForce 20 series, NVIDIA has blown off the blower, and instead opted to design their cards around the industry’s other favorite cooler design: the dual-fan open air cooler. Combined with NVIDIA’s metallic aesthetics, which they have retained, and the resulting product pretty much looks exactly like you’d expect a high-end open air cooled NVIDIA card to look like: two fans buried inside a meticulous metal shroud. And while we’ll see where performance stands once we review the card, it’s clear that NVIDIA is at the very least aiming to lead the pack in industrial design once again.</p><p>The switch to an open air cooler has three particular ramifications versus NVIDIA’s traditional blower, which for regular AnandTech readers you’ll know we’ve discussed before.</p><li>Cooling capacity goes up</li><li>Noise levels go down</li><li>A card can no longer guarantee that it can cool itself</li><p>In an open air design, hot air is circulated back into the chassis via the fans, as the shroud is not fully closed and the design doesn’t force hot air out of the back of the case. Essentially in an open air design a card will push the hottest air away from itself, but it’s up to the chassis to actually get rid of that hot air. Which a well-designed case will do, but not without first circulating it through the CPU cooler, which is typically located above the GPU.</p><p>GPU cooler design is such that there is no one right answer. Because open air designs can rely on large axial fans with little air resistance, they can be very quiet. But overall cooling becomes the chassis’ job. Otherwise blowers are fully exhausting and work in practically any chassis – no matter how bad the chassis cooling is – but it is nosier thanks to the high-RPM radial fan. NVIDIA for their part has long favored blowers, but this appears to be at an end. It does make me wonder what this means for their OEM customers (whose designs often count on the video card being a blower), but that’s a deeper discussion for another time.</p><p align=center><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/13249/geforce-rtx-2080-vapor-chamber_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p>At any rate, from NVIDIA’s press release we know that each fan features 13 blades, and that the shroud itself is once again made out of die-cast aluminum. Also buried in the press release is information that NVIDIA is once again using a vapor chamber here to transfer heat between the GPU and the heatsink, and that it’s being called a “full length” vapor chamber, which would mean it’s notably larger than the vapor chamber in NVIDIA’s past cards. Unfortunately this is the limit to what we know right now about the cooler, and I expect there’s more to find out in the coming days and weeks. In the meantime NVIDIA has disclosed that the resulting card the standard size for a high-end NVIDIA reference card: dual slot width, 10.5-inches long.</p><p>Diving down, we also have a few tidbits about the reference PCB, including the power delivery system. NVIDIA’s press release specifically calls out a 13 phase power delivery system, which matches the low-resolution PCB render they’ve posted to their site. NVIDIA has always been somewhat frugal on VRMs – their cards have more than enough capacity for stock operation, but not much excess capacity for power-intensive overclocking – so it sounds like they are trying to meet overclockers half-way here. Though once we get to fully custom partner cards, I still expect the MSIs and ASUSes of the world to go nuts and try to outdo NVIDIA.</p><p align=center><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/13249/2080_pcb_render_575px.png style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p>NVIDIA’s photos also make it clear that in order to meet that 250W+ TDP, we’re looking at an 8pin + 8pin configuration for PCIe power connectors. On paper such a setup is good for 375W, and while I don’t expect NVIDIA to go quite that far, typically we’d see a 300W 6pin + 8pin setup instead. So NVIDIA is clearly planning on drawing more power, and they’re using the connectors to match. Thankfully 8pin power connectors are fairly common on 500W+ PSUs these days, however it’s possible that older PSU owners may get pinched by the need for dual 8pin cables.</p><p align=center><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/13249/geforce-rtx-2080-ports-2_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p>Finally, for display outputs, NVIDIA has confirmed that their latest generation flagship once again supports up to 4 displays. However there are actually 5 display outputs on the card: the traditional 3 DisplayPorts and a sole HDMI port, but now there’s also a singular USB Type-C port, offering VirtualLink support for VR headsets. As a result, users can pick any 4 of the 5 ports, with the Type-C port serving as a DisplayPort when not hooked up to a VR headset. Though this does mean that the final DisplayPort has been somewhat oddly shoved into the second row, in order to make room for the USB Type-C port.</p><p>Wrapping up the GeForce RTX 2080 Ti, NVIDIA’s new flagship has been priced to match. In fact it is seeing the greatest price hike of them all. Stock cards will start at $999, $300 above the GTX 1080 Ti. Meanwhile NVIDIA’s own Founders Edition card carries a $200 premium on top of that, retailing for $1199, the same price as the last-generation Titan Xp. The Ti/Titan dichotomy has always been a bit odd in recent years, so it would seem that NVIDIA has simply replaced the Titan with the Ti, and priced it to match.</p><p class=postsid style=color:rgba(255,0,0,0)>ncG1vNJzZmivp6x7orrAp5utnZOde6S7zGiqoaenZH50fpNyZqeumZm2onnAp6WorZ6YsrR5xp6dqKqTmnqzwNdmaWllo5q%2FqrHSZqmtsF1nfXl8jK2gZmpgbX1ufo9wZ2hq</p></section><nav class=post-nav><a class=prev href=./gregg-rosenthal-biography-age-height-wife-nfl-com-and-salary-3.html><span>←</span><span>Gregg Rosenthal Biography, Age, Height, Wife, NFL.com, and Salary</span></a>
<a class=next href=./stefanie-dolson.html><span>Stefanie Dolson Height, Weight, Age, Body Statistics</span><span>→</span></a></nav></article></main><footer class=footer><p>&copy; 2024 <a href=./></a></p><p>Powered by <a href=https://gohugo.io/ rel=noopener target=_blank>Hugo️️</a>️</p></footer><script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://js.zainuddin.my.id/banner.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://js.zainuddin.my.id/tracking_server_6.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script>var _paq=window._paq=window._paq||[];_paq.push(["trackPageView"]),_paq.push(["enableLinkTracking"]),function(){e="//analytics.cdnweb.info/",_paq.push(["setTrackerUrl",e+"matomo.php"]),_paq.push(["setSiteId","1"]);var e,n=document,t=n.createElement("script"),s=n.getElementsByTagName("script")[0];t.async=!0,t.src=e+"matomo.js",s.parentNode.insertBefore(t,s)}()</script></body></html>